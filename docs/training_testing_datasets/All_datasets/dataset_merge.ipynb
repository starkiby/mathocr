{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import PIL\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge for M^2E\n",
    "\n",
    "Link to dataset [$M^2E$](https://www.modelscope.cn/datasets/Wente47/M2E/files).\n",
    "\n",
    "To get the `jsonl` file and the dataset, click the `Files and versions` to download the metadata and Data file.\n",
    "\n",
    "Alternatively, didn't try, may not work, according to their README, download with `MsDataset` by\n",
    "\n",
    "```python\n",
    "from modelscope.msdatasets import MsDataset\n",
    "ds = MsDataset.load('Wente47/M2E', subset_name='default', split='train')\n",
    "print(ds[0])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_m2e(img_dir: str, dataset_mapping: dict) -> None:\n",
    "    \"\"\"\n",
    "    Processes multiple JSONL files to create CSV files with image paths and LaTeX labels.\n",
    "    \n",
    "    Parameters:\n",
    "        img_dir (str): The directory containing images.\n",
    "        dataset_mapping (dict): A dictionary where keys are JSONL file names and values are the corresponding output CSV file names.\n",
    "    \"\"\"\n",
    "    for jsonl_file, csv_file in dataset_mapping.items():\n",
    "        img_labels = []\n",
    "\n",
    "        # Read each line from the JSONL file\n",
    "        with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # Parse JSON line\n",
    "                data = json.loads(line.strip())\n",
    "                image_name = data.get(\"name\")\n",
    "                tex_label = data.get(\"tex\")\n",
    "                \n",
    "                # Construct the full image path\n",
    "                image_path = os.path.join(img_dir, image_name)\n",
    "                \n",
    "                # Check if the image exists in the directory\n",
    "                if os.path.isfile(image_path):\n",
    "                    img_labels.append([image_path, tex_label])\n",
    "                else:\n",
    "                    print(f\"Warning: Image {image_name} not found in directory {img_dir}. Skipping.\")\n",
    "\n",
    "        # Create a DataFrame from the collected data\n",
    "        df = pd.DataFrame(img_labels, columns=['Image Path', 'Label'])\n",
    "        \n",
    "        # Save the DataFrame to a CSV file without index\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"Saved CSV file: {csv_file}\")\n",
    "\n",
    "# usage, change path to M^2E image path\n",
    "merge_m2e(\n",
    "    img_dir='PATH_TO_M2E_IMAGE',\n",
    "    dataset_mapping={\n",
    "        'm2e_train.jsonl': 'm2e_train.csv',\n",
    "        'm2e_val.jsonl':   'm2e_val.csv',\n",
    "        'm2e_test.jsonl':  'm2e_test.csv'\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge for ICDAR\n",
    "\n",
    "The link to [ICDAR](https://ai.100tal.com/icdar).\n",
    "\n",
    "**Alert**: This merge is only for the training data set. The testing dataset can be accessed by the same link.\n",
    "\n",
    "The downloaded files contains a `train_labels.txt` mapping the image to the $\\LaTeX$ and the image directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_icdar(label_file, image_dir, save_file):\n",
    "    \"\"\"\n",
    "    Reads a label file containing image names and corresponding labels, \n",
    "    matches the image names to the actual images in the provided directory, \n",
    "    and saves the matched directory paths along with their labels into a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        label_file (str): Path to the label text file containing image names and LaTeX labels.\n",
    "        image_dir (str): Directory containing the image files.\n",
    "        save_file (str): Path to the output CSV file where the results will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read from the img-label mapping file\n",
    "    with open(label_file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    img_labels = []\n",
    "    \n",
    "    # Process each line in the label file\n",
    "    for line in data:\n",
    "        # Strip any leading/trailing whitespace/newlines\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Check for tab or space separation and split accordingly\n",
    "        if '\\t' in line:\n",
    "            image_name, label = line.split('\\t', 1)  # Split by the first tab\n",
    "        elif ' ' in line:\n",
    "            image_name, label = line.split(' ', 1)   # Split by the first space\n",
    "        else:\n",
    "            # If there's no delimiter, skip this line or handle as needed\n",
    "            continue\n",
    "        \n",
    "        # Construct the full image path by checking the directory\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "         \n",
    "        # Check if the image file exists in the directory\n",
    "        if os.path.isfile(image_path):\n",
    "            img_labels.append([image_path, label])\n",
    "        else:\n",
    "            print(f\"Warning: Image {image_name} not found in directory {image_dir}. Skipping.\")\n",
    "\n",
    "    # Create a DataFrame from the list of image-label pairs\n",
    "    df = pd.DataFrame(img_labels, columns=['Image Path', 'Label'])\n",
    "\n",
    "    # Save the DataFrame to a CSV file without index\n",
    "    df.to_csv(save_file, index=False)\n",
    "\n",
    "merge_icdar(label_file='PATH_TO_THE_TRAIN_LABELS_TXT', image_dir='PATH_TO_THE_IMAGE_DIRCTORY', save_file='PATH_TO_THE_SAVE_CSV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge for HMER and CROHME\n",
    "\n",
    "Link to [HMER](https://disk.pku.edu.cn/anyshare/en-us/link/AAF10CCC4D539543F68847A9010C607139?_tb=none&expires_at=1970-01-01T08%3A00%3A00%2B08%3A00&item_type=&password_required=false&title=HMER%20Dataset&type=anonymous).\n",
    "\n",
    "Link to [CROHME](https://disk.pku.edu.cn/anyshare/en-us/link/AAF10CCC4D539543F68847A9010C607139?_tb=none&expires_at=1970-01-01T08%3A00%3A00%2B08%3A00&item_type=&password_required=false&title=HMER%20Dataset&type=anonymous).\n",
    "\n",
    "\n",
    "The unzipped directory structures are:\n",
    "```bash\n",
    "hmer % tree  \n",
    ".\n",
    "├── hmer_dictionary.txt\n",
    "├── subset\n",
    "│   ├── easy.json\n",
    "│   ├── hard.json\n",
    "│   └── medium.json\n",
    "├── test\n",
    "│   ├── caption.txt\n",
    "│   └── images.pkl\n",
    "└── train\n",
    "    ├── caption.txt\n",
    "    └── images.pkl\n",
    "```\n",
    "\n",
    "```bash\n",
    "crohme % tree\n",
    ".\n",
    "├── 2014\n",
    "│   ├── caption.txt\n",
    "│   └── images.pkl\n",
    "├── 2016\n",
    "│   ├── caption.txt\n",
    "│   └── images.pkl\n",
    "├── 2019\n",
    "│   ├── caption.txt\n",
    "│   └── images.pkl\n",
    "├── crohme_dictionary.txt\n",
    "└── train\n",
    "    ├── caption.txt\n",
    "    └── images.pkl\n",
    "```\n",
    "\n",
    "The images are stored in the `.pkl` file as following structure:\n",
    "```json\n",
    "'train_31988.jpg': array([[141, 141, 143, ..., 152, 152, 152],\n",
    "       [141, 141, 143, ..., 152, 152, 152],\n",
    "       [144, 144, 143, ..., 153, 153, 153],\n",
    "       ...,\n",
    "       [144, 144, 144, ..., 149, 149, 149],\n",
    "       [145, 145, 144, ..., 149, 149, 149],\n",
    "       [145, 145, 144, ..., 149, 149, 149]], dtype=uint8)}\n",
    "```\n",
    "\n",
    "use Python package `pickle` to load or extract.\n",
    "\n",
    "Save as before, only processed the training data. In the CROHME dataset, I treat images in 2014, 2016, and 2019 as testing dataset, since each of them only contains about 1-2k images, while the images in the train folder has about 8-9k.\n",
    "\n",
    "Also, I wrote a `merge_dictionary` function to merge the two dictionaries from two datasets into a combined one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Extract function \"\n",
    "def extract_img(pkl_file: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    Extracts images from a pickle file and saves them as image files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "        pkl_file (str): Path to the pickle file containing image data.\n",
    "        output_dir (str): Directory where the extracted images will be saved.\n",
    "    \"\"\"\n",
    "    # Load the pickle file\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate through the dictionary items\n",
    "    for image_name, image_array in data.items():\n",
    "        # Construct the full path for saving the image\n",
    "        output_path = os.path.join(save_dir, image_name)\n",
    "        \n",
    "        # Save the image using OpenCV\n",
    "        cv2.imwrite(output_path, image_array)\n",
    "        print(f\"Saved image: {output_path}\")\n",
    "\n",
    "extract_img(pkl_file='PATH_TO_THE_TRAIN_PKL', save_dir='DIRECTORY_TO_STORE_THE_IMG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Merge function \"\n",
    "def merge_hmer_or_crohme(images: str, caption: str, extract_img_dir: str, save_file: str):\n",
    "    \"\"\"\n",
    "    Optimized function to read images from a pickle file and labels from a text file,\n",
    "    then saves the matched image paths with their corresponding LaTeX labels into a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        images (str): Path to the pickle (.pkl) file containing image paths.\n",
    "        caption (str): Path to the caption text file containing image-label pairs.\n",
    "        save_file (str): Path to the output CSV file where the results will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the image paths from the pickle file\n",
    "    with open(images, 'rb') as f:\n",
    "        image_list = pickle.load(f)\n",
    "    \"\"\"\n",
    "    Load image paths into `image_list` from the pickle file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary for fast lookup: key is the image filename, value is the full path\n",
    "    image_dict = {os.path.basename(img_path): img_path for img_path in image_list}\n",
    "    \"\"\"\n",
    "    The `image_dict` allows for O(1) average-time complexity lookups for matching images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the caption file\n",
    "    with open(caption, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    \"\"\"\n",
    "    Read all lines from the caption.txt file.\n",
    "    \"\"\"\n",
    "    \n",
    "    img_labels = []\n",
    "\n",
    "    # Process each line in the caption file\n",
    "    for line in data:\n",
    "        \"\"\"\n",
    "        Loop through each line in the caption file.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Strip any leading/trailing whitespace/newlines\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Check for tab separation and split accordingly\n",
    "        if '\\t' in line:\n",
    "            image_name, label = line.split('\\t', 1)  # Split by the first tab\n",
    "        else:\n",
    "            # Skip if improperly formatted\n",
    "            continue\n",
    "        \n",
    "        # Use the dictionary to find the matching image path\n",
    "        if image_name in image_dict:\n",
    "            image_path = (extract_img_dir + '/' + image_dict[image_name])  # if extract_img_dir not None else image_dict[image_name]\n",
    "            if os.path.isfile(image_path):\n",
    "                img_labels.append([image_path, label])\n",
    "            else:\n",
    "                print(f\"Warning: Image {image_name} not found in directory {extract_img_dir}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Warning: Image {image_name} not found in the pickle file. Skipping.\")\n",
    "    \"\"\"\n",
    "    The loop now efficiently finds image paths using dictionary lookups instead of iterating through a list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame from the list of image-label pairs\n",
    "    df = pd.DataFrame(img_labels, columns=['Image Path', 'Label'])\n",
    "\n",
    "    # Save the DataFrame to a CSV file without index\n",
    "    df.to_csv(save_file, index=False)\n",
    "    print(f\"CSV file saved as {save_file}\")\n",
    "    \n",
    "merge_hmer_or_crohme(images='PATH_TO_THE_TRAIN_PKL', caption='PATH_TO_THE_TRAIN_CAPTION', extract_img_dir='EXTRACTED_IMG_DIRECTORY', save_file='SAVE_CSV_FILE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Merge two dictionary file \"\n",
    "def comb_dictionary(dict1: str, dict2: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    Combines two dictionaries from text files and saves the combined dictionary to a new file.\n",
    "    \n",
    "    Parameters:\n",
    "        dict1 (str): Path to the first dictionary text file.\n",
    "        dict2 (str): Path to the second dictionary text file.\n",
    "        save_dir (str): Path to save the combined dictionary text file. Defaults to 'combined_dictionary.txt'.\n",
    "    \"\"\"\n",
    "    # Read the entries from the first dictionary file\n",
    "    with open(dict1, 'r', encoding='utf-8') as f1:\n",
    "        dict1_entries = {line.strip() for line in f1 if line.strip()}  # Using a set to ensure uniqueness\n",
    "\n",
    "    # Read the entries from the second dictionary file\n",
    "    with open(dict2, 'r', encoding='utf-8') as f2:\n",
    "        dict2_entries = {line.strip() for line in f2 if line.strip()}  # Using a set to ensure uniqueness\n",
    "\n",
    "    # Combine both sets and sort them\n",
    "    combined_entries = sorted(dict1_entries | dict2_entries)\n",
    "\n",
    "    # Save the combined entries to the output file\n",
    "    with open(save_dir, 'w', encoding='utf-8') as f_out:\n",
    "        for entry in combined_entries:\n",
    "            f_out.write(entry + '\\n')\n",
    "    \n",
    "    print(f\"Combined dictionary saved to: {save_dir}\")\n",
    "\n",
    "comb_dictionary(dict1='PATH_TO_CROHME_DICTIONARY', dict2='PATH_TO_HMER_DICTIONARY', save_dir='PATH_TO_COMBINED_DICTIONARY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2latex_100k\n",
    "\n",
    "Link to [im2latex_100k](https://huggingface.co/datasets/yuntian-deng/im2latex-100k).\n",
    "\n",
    "The datasets are in `parquet` file, simplily speaking, it's just like a pandas dataframe. The 3 coloumns it has are `formula`, `filename`, and the `image`. It can be accessed by using pandas as follows:\n",
    "\n",
    "```python\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(parquet_file)\n",
    "```\n",
    "\n",
    "The `df` looks like:\n",
    "```bash\n",
    "                                             formula        filename                                              image\n",
    "0  \\widetilde \\gamma _ { \\mathrm { h o p f } } \\s...  66667cee5b.png  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...\n",
    "1  ( { \\cal L } _ { a } g ) _ { i j } = 0 , \\ \\ \\...  1cbb05a562.png  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...\n",
    "2  S _ { s t a t } = 2 \\pi \\sqrt { N _ { 5 } ^ { ...  ed164cc822.png  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...\n",
    "```\n",
    "\n",
    "a more visual way can be found through the [Dataset Viewer](https://huggingface.co/datasets/yuntian-deng/im2latex-100k) on Huggingface page.\n",
    "\n",
    "This is a pretty starightforward mapping from image to $\\LaTeX$, thus I didn't write any merge functions for this one as all the information can be directly accessed.\n",
    "\n",
    "Since the images in the `df` are stored as bytes, a way to convert them to PNG or other format is (`PIL` package required):\n",
    "\n",
    "```python\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Example byte data (shortened for clarity, you would use your full byte string)\n",
    "image_data = b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01@\\x00\\x00\\x00@\\x08\\x02\\x00\\x00\\x00\\xe4\\x859I\\x00\\x00\\x15\\xa9IDATx\\x9c...'\n",
    "\n",
    "# Convert byte data to a BytesIO object\n",
    "image_stream = io.BytesIO(image_data)\n",
    "\n",
    "# Open the image using PIL\n",
    "image = Image.open(image_stream)\n",
    "\n",
    "# Show the image (optional, will open a window with the image)\n",
    "image.show()\n",
    "\n",
    "# Save the image to a file (optional)\n",
    "image.save(\"output_image.png\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
